{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 텍스트 전처리\n",
    "---\n",
    "- 패키지 설치\n",
    "    - nltk : pip install nltk\n",
    "    - KoNLPy : pip install KoNLPy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [1] 토큰화(Tokenization)\n",
    "- 문장/문서를 의미를 지닌 작은 단위로 나누는 것\n",
    "- 나누어진 단어를 토큰(Token)이라고 함\n",
    "- 종류\n",
    "    - 문장 토큰화\n",
    "    - 단어 토큰화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all', quiet=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "raw_text_1 = \"We can also operate at the level of sentences, using the sentence tokenizer directly as follows\"\n",
    "raw_text_2 = \"Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'can', 'also', 'operate', 'at', 'the', 'level', 'of', 'sentences', ',', 'using', 'the', 'sentence', 'tokenizer', 'directly', 'as', 'follows']\n",
      "['Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.', 'For', 'example', ',', 'tokenizers', 'can', 'be', 'used', 'to', 'find', 'the', 'words', 'and', 'punctuation', 'in', 'a', 'string']\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 토큰화\n",
    "result_1 = word_tokenize(raw_text_1)\n",
    "result_2 = word_tokenize(raw_text_2)\n",
    "\n",
    "print(result_1)\n",
    "print(result_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 문장 단위 토큰화\n",
    "raw_test_3 = raw_text_1 + ' ' + raw_text_2\n",
    "sent_result = sent_tokenize(raw_test_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "['We can also operate at the level of sentences, using the sentence tokenizer directly as follows Tokenizers divide strings into lists of substrings.',\n 'For example, tokenizers can be used to find the words and punctuation in a string']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}